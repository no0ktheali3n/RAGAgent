# ðŸ§  RAGAgent Tutorial â€” Retrieval-Augmented Generation with LangChain

RAGAgent demonstrates a **modular Retrieval-Augmented Generation (RAG)** pipeline built with **LangChain**, **Chroma**, and **Hugging Face embeddings**, with optional integration for **OpenAI** and **AWS Bedrock / Anthropic Claude** models.

This project is the baseline for the upcoming **ContextAI** system, which will extend RAGAgent to index and reason over Wikipedia and larger corpora.

---

## ðŸš€ Overview

RAGAgent evolves from a simple semantic search indexer into a full RAG workflow capable of **document retrieval** and **grounded generation**.

### System Components

1. **Indexer** â€” Loads, chunks, embeds, and persists documents locally using Chroma.  
2. **Validator** â€” Verifies data integrity and semantic search performance.  
3. **RAG Engine** â€” Retrieves relevant chunks and synthesizes an answer using an LLM.

### RAG Flow Diagram

~~~
Question
   â”‚
   â–¼
Retriever (Chroma Vector Store)
   â”‚ â†’ Top-k similar chunks
   â–¼
Context Formatter
   â”‚ â†’ Annotates context with chunk indices
   â–¼
Prompt Template (System + User)
   â”‚
   â–¼
LLM (OpenAI or Bedrock)
   â”‚
   â–¼
Final Grounded Answer
~~~

---

## ðŸ“ Project Structure

~~~
RAGAgent/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ main.py         # CLI entrypoint (validate | rag)
â”‚   â”œâ”€â”€ indexer.py      # Loads, chunks, embeds, and persists documents
â”‚   â””â”€â”€ rag.py          # Implements Retrieve â†’ Augment â†’ Generate chain
â”‚
â”œâ”€â”€ data/chroma/        # Chroma vector store persistence
â”œâ”€â”€ .env                # Provider credentials and environment configuration
â””â”€â”€ requirements.txt
~~~

---

## ðŸ§± Dependencies

> **Note:** This project uses [`uv`](https://github.com/astral-sh/uv) â€” a fast Python package manager and runtime for dependency management and script execution.  
> You must have `uv` installed before running this project. Choose one of the following installation methods:

### ðŸªŸ Windows

**Option 1: Chocolatey (Recommended)**
~~~powershell
choco install uv
~~~

**Option 2: PowerShell (Official Installer)**
~~~powershell
iwr https://astral.sh/uv/install.ps1 -UseBasicParsing | iex
~~~

**Verify Installation**
~~~powershell
uv --version
~~~

### ðŸ§ macOS / Linux

~~~bash
curl -LsSf https://astral.sh/uv/install.sh | sh
~~~

**Verify Installation**
~~~bash
uv --version
~~~

Install core packages:

~~~bash
uv add langchain-core langchain-community langchain-text-splitters langchain-huggingface langchain-chroma langchain-aws chromadb
~~~

Optional OpenAI:

~~~bash
uv add langchain-openai
~~~

---

## âš™ï¸ Configuration

Create a `.env` file in the project root with the following keys:

~~~powershell
# Embeddings
EMBED_MODEL=BAAI/bge-small-en-v1.5
CHROMA_DIR=./data/chroma
COLLECTION=RAGAgent_tutorial

# OpenAI (optional)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini
OPENAI_TEMPERATURE=0.0

# AWS Bedrock (optional)
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
BEDROCK_MODEL=global.anthropic.claude-haiku-4-5-20251001-v1:0
BEDROCK_REGION=us-east-1
BEDROCK_TEMPERATURE=0.0

# LangSmith (optional)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=...
LANGCHAIN_PROJECT=RAGAgent
~~~

> **Note:** Bedrock uses standard AWS credentials (IAM/SigV4). The Bedrock â€œAPI keyâ€ mechanism is not used by LangChainâ€™s `ChatBedrock`.

---

## ðŸ§ª Usage

### 1) Validate Mode â€” Build and Inspect the Index

Validates document ingestion, chunking, and persistence while running a simple semantic search test.

~~~powershell
uv run python scripts/main.py --validate
~~~

Expected highlights:

~~~
âœ… Loaded document â€” 43,047 characters
âœ… Split into 63 chunks
âœ… Vector store currently holds 63 embeddings
ðŸ”Ž Retrieval test for query: 'What is Task Decomposition?'
~~~

### 2) RAG Mode â€” Retrieve and Generate an Answer

Runs the complete RAG chain using your chosen LLM provider.

**Bedrock:**
~~~bash
uv run python scripts/main.py --rag --provider bedrock --question "How do agents plan and decompose tasks?"
~~~

**OpenAI:**
~~~bash
uv run python scripts/main.py --rag --provider openai --question "Explain the difference between planning and reacting in agents."
~~~

**Optional: override retrieval depth (k):**
~~~bash
uv run python scripts/main.py --rag --question "What is task decomposition?" --k 5
~~~

---

## ðŸ”§ Internals

### `scripts/indexer.py`
Responsibilities:
- **Load** web content with `WebBaseLoader` (custom User-Agent to avoid 403s)  
- **Split** content using `RecursiveCharacterTextSplitter` (overlap keeps coherence)  
- **Embed** chunks with `HuggingFaceEmbeddings` (`BAAI/bge-small-en-v1.5`)  
- **Persist** vectors to **Chroma** on disk  
- **Idempotence**: skips re-ingest if vectors already exist, but still re-loads/splits for validation

### `scripts/rag.py`
Implements RAG:
- **Lazy, thread-safe init**: builds/loads the vector store on first use (no heavy work at import; guarded by a `Lock`)  
- **Context formatter**: converts retrieved `Document` chunks into a single block with inline labels like `(chunk_index=##)` for transparent citation  
- **Runnable chain**:
  ~~~
  {"question"} â†’ retriever(k) â†’ _format_context â†’ PROMPT â†’ llm â†’ StrOutputParser()
  ~~~
- **k override**: default `k=3`, optionally overridden at runtime (`--k N`)

### `scripts/main.py`
CLI controller:
- `--validate` â†’ ingestion/chunking/persistence checks + semantic search  
- `--rag` â†’ runs full Retrieve â†’ Augment â†’ Generate answer  
- `--provider` â†’ `"openai"` or `"bedrock"` (auto-detects if omitted)  
- `--k` â†’ number of retrieved chunks  
- `_make_llm()` factory builds either:
  - `ChatOpenAI` (OpenAI) or
  - `ChatBedrock` (AWS Bedrock; supports Claude Haiku 4.5)

---

## ðŸŽ›ï¸ Tuning Cheat-Sheet (RAG vs Generation)

- **Retrieval `k` (rag.py)**: how many **chunks** to retrieve from Chroma. More can improve coverage but may add noise.  
- **Temperature (LLM)**: randomness of **wording**. For faithful RAG, keep **`0.0`** (deterministic).  
- **Top-p / Top-k (LLM sampling)**: only matter when temperature > 0. For creative or multi-answer tasks, try `temp=0.3, top_p=0.9`.

---

## ðŸ§® Version Summary

| Version | Highlights |
|--------:|------------|
| **v0.2.0** | Implemented full RAG pipeline (Retrieve â†’ Augment â†’ Generate). Added Bedrock + OpenAI LLM support, LangSmith hooks, lazy & thread-safe vector store init, and `--k` override. |
| **v0.1.1** | Refactor for modularity; introduced `scripts/main.py` CLI with `--validate` and `--rag`; provider factory; `.env` configuration. |
| **v0.1.0** | Initial indexer: web loading, chunking, embeddings, local Chroma persistence, and semantic search validation. |

---

## ðŸ§­ Roadmap

- [ ] Agent / Tool integration
- [ ] Wikipedia ingestion for **ContextAI**  
- [ ] LangSmith tracing + prompt metadata  
- [ ] Reranking / hybrid search  
- [ ] RAG evaluation (RAGAS / TruLens)  
- [ ] Caching + parallelization for throughput

---

## ðŸ“œ License

**MIT License** â€” free to use, modify, and extend.

---

## ðŸ’¡ Acknowledgements

- [LangChain](https://www.langchain.com/) â€” Framework for building RAG and agentic systems  
- [Chroma](https://www.trychroma.com/) â€” Local vector database  
- [Hugging Face](https://huggingface.co/) â€” Embedding models (BAAI/bge-small-en-v1.5)  
- [Anthropic Claude](https://www.anthropic.com/) â€” LLM via AWS Bedrock  
- [OpenAI GPT](https://platform.openai.com/docs/models) â€” Optional model backend
